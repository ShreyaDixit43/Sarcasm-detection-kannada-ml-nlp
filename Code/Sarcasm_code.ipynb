{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137df00-b161-4766-b9cf-24848757c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Use cu118 if you have a compatible NVIDIA GPU, otherwise remove it for CPU only\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "# Restart the kernel after running this cell to ensure changes take effect.\n",
    "# Then, import necessary libraries:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# -- coding: utf-8 --\n",
    "# -- coding: utf-8 --\n",
    "!pip install xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import codecs\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as XGBoost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "#Import and download 'punkt' resource from nltk\n",
    "import nltk\n",
    "# Download the necessary resources\n",
    "nltk.download('punkt')  # Download the 'punkt' resource for tokenization\n",
    "nltk.download('punkt_tab') # Download Punkt Tokenizer Models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import torch\n",
    "#Import necessary libraries for BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# Instead of importing get_linear_schedule_with_warmup directly, import optimization\n",
    "from transformers import optimization\n",
    "#Import AdamW from transformers\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\"\"\"#Reading Dataset\"\"\"\n",
    "df = pd.read_excel('Trainfile.xlsx')\n",
    "\n",
    "\n",
    "\"\"\"#Data Cleaning\"\"\"\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    stopwords = codecs.open(\"stopwords.txt\", \"r\", encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    stopword = []\n",
    "    for line in stopwords:\n",
    "        data1 = line.replace('\\r', \"\")\n",
    "        stopword.append(data1)\n",
    "\n",
    "    lexicon = []\n",
    "    all_words = word_tokenize(data)\n",
    "    exclude = set(string.punctuation)\n",
    "    for i in all_words:\n",
    "        st = ''.join(ch for ch in i if ch not in exclude)\n",
    "        if (st != ''):\n",
    "            lexicon.append(st)\n",
    "    lexicons = []\n",
    "    for word in lexicon:\n",
    "        if not word in stopword:\n",
    "            lexicons.append(word)\n",
    "    lexi = ' '.join([str(elem) for elem in lexicons])\n",
    "    return lexi\n",
    "\n",
    "\"\"\"#Stemming\"\"\"\n",
    "complex_suffixes = {\n",
    "    # ... (your existing stemming rules)\n",
    "# PAST TENSE simple past tense 1st person singular\n",
    "    1: [\"ಳಿದ್ದೆ\", \"ಳಲಿಲ್ಲ\", \"ಳಿದ್ದೆನ\", \"ಳಿದೆನ\"],  # ---> append ಳು\n",
    "\n",
    "    # simple past tense 1st person plural\n",
    "    2: [\"ದಿದೆವು\", \"ದಲಿಲ್ಲ\", \"ದಿದೆವ\"],  # ---> append ದು\n",
    "\n",
    "    # simple past tense 2nd person\n",
    "    3: [\"ಯಲಿಲ್ಲ\"],\n",
    "\n",
    "    # simple past tense 3rd person plural\n",
    "    4: [\"ಯಾಗಿದ್ದರು\", \"ವಾಗಿದ್ದರು\", \"ತಾಗಿದ್ದರು\", \"ದಾಗಿದ್ದರು\", \"ದಿದ್ದರು\", \"ಲಿಲ್ಲ\", \"ದ್ದರಾ\"],\n",
    "\n",
    "    # simple past tense 3rd person singular\n",
    "    5: [\"ಯಲಿಲ್ಲ\", \"ಲಿಲ್ಲ\", \"ದನ\", \"ದನಾ\"],\n",
    "\n",
    "    # past perfect tense 1st person singular\n",
    "    6: [\"ದಿದ್ದೆ\", \"ಡಿದ್ದೆ\", \"ರಲಿಲ್ಲ\", \"ದ್ದೆನ\", \"ದ್ದೆನಾ\"],\n",
    "\n",
    "    # past perfect tennse 1st person plural\n",
    "    7: [\"ದಿದ್ವಿ\", \"ರಲಿಲ್ಲ\", \"ದಿದ್ವಾ\"],\n",
    "\n",
    "    # past perfect, 2nd\n",
    "    8: [\"ದಿದ್ದೆ\", \"ಯುತ್ತಿದ್ದೆ\", \"ತ್ತಿದ್ದವರು\", \"ತ್ತಿದ್ದೆ\", \"ತಿದ್ದೆ\", \"ಯುತ್ತದೆ\", \"ತ್ತದೆ\", \"ಯುತ್ತಿರಲಿಲ್ಲ\", \"ತ್ತಿರಲಿಲ್ಲ\",\n",
    "        \"ತಿರಲಿಲ್ಲ\", \"ದಿರಲಿಲ್ಲ\", \"ದ್ದಿದ್ದಾ\", \"ಯುತ್ತಿದ್ದಾ\", \"ತ್ತಿದ್ದಾ\"],\n",
    "\n",
    "    # past perfect 3rd plural\n",
    "    9: [\"ದಿದ್ದರು\"],\n",
    "\n",
    "    # past perfect 3rd singular\n",
    "    10: [\"ದಿದ್ದ\", \"ದಿದ್ದನು\", \"ದಿದ್ದಳು\"],\n",
    "\n",
    "    # PAST CONTINUOUS simple tense 1st singular\n",
    "    11: [\"ತ್ತಿದ್ದೆನೆ\"],\n",
    "\n",
    "    # past continuous 1st plural\n",
    "    12: [\"ಯುತ್ತಿದ್ದೆವು\", \"ತ್ತಿದ್ದೆವು\", \"ಯುತ್ತಿದ್ದೆವ\", \"ತ್ತಿದ್ದೆವ\"],\n",
    "\n",
    "    # past continuous 2nd\n",
    "    13: [\"ತ್ತಿದ್ದೆ\", \"ತಿರಲಿಲ್ಲ\", \"ತ್ತಿದ್ದ\", \"ತ್ತಿದ್ದಾ\"],\n",
    "\n",
    "    # past continuous 3rd plural\n",
    "    14: [\"ತ್ತಿದ್ದರು\", \"ತ್ತಿರಲಿಲ್ಲ\", \"ತ್ತಿದ್ದರ\", \"ತ್ತಿದ್ದಾರಾ\"],\n",
    "\n",
    "    # past continuous 3rd singular\n",
    "    15: [\"ಯುತ್ತಿದ್ದನ\", \"ಯುತ್ತಿದ್ದನಾ\", \"ಯುತ್ತಿದ್ದಳು\", \"ಯುತ್ತಿದ್ದನು\", \"ಯುತ್ತಿದ್ದಳ\", \"ಯುತ್ತಿದ್ದನ\", \"ಯುತ್ತಿದ್ದಳೆ\",\n",
    "         \"ಯುತ್ತಿದ್ದನೆ\", \"ತ್ತಿದ್ದನ\", \"ತ್ತಿದ್ದನಾ\", \"ತ್ತಿದ್ದಳು\", \"ತ್ತಿದ್ದನು\", \"ತ್ತಿದ್ದಳ\", \"ತ್ತಿದ್ದನ\", \"ತ್ತಿದ್ದಳೆ\",\n",
    "         \"ತ್ತಿದ್ದನೆ\"],\n",
    "\n",
    "    # PAST PERFECT continuous 1st singular\n",
    "    16: [\"ತ್ತಿದ್ದೆ\", \"ತ್ತಿರಲಿಲ್ಲ\", \"ತ್ತಿದ್ದೆನ\", \"ತ್ತಿದ್ದೆನಾ\"],\n",
    "\n",
    "    # past perfect continuous 1st plural\n",
    "    17: [\"ಯುತ್ತಿದ್ದೆವೆ\", \"ತ್ತಿದ್ದೆವೆ\", \"ಯುತ್ತಿದ್ದೆವು\", \"ತ್ತಿದ್ದೆವು\"],\n",
    "\n",
    "    # past p continous 2nd\n",
    "    18: [\"ತ್ತಿದ್ದೆ\", \"ತ್ತಿದ್ದೆವು\", \"ತ್ತಿರಲಿಲ್ಲ\", \"ತ್ತಿದ್ದಾ\"],  # ----- not needed\n",
    "\n",
    "    # past p continuous 3rd plural\n",
    "    19: [\"ತ್ತಿದ್ದರು\", \"ತ್ತಿದ್ದರು\"],  # -------- not needed\n",
    "\n",
    "    # past p continuous 3rd singular\n",
    "    20: [\"ತ್ತಿಲ್ಲ\", \"ತ್ತಿದ್ದಳ\", \"ತ್ತಿದ್ದಳು\", \"ತ್ತಿದ್ದನ\", \"ತ್ತಿದ್ದನು\", \"ತ್ತಿಸಿದ್ದಾರೆ\"],\n",
    "\n",
    "    # PRESENT TENSE\n",
    "    # simple 1st singular\n",
    "    21: [\"ರುತ್ತೆನೆ\", \"ತ್ತೆನೆ\", \"ದಿಲ್ಲ\", \"ಯಲ್ವಾ\"],\n",
    "\n",
    "    # simple 1st plural\n",
    "    22: [\"ರುತ್ತೆವೆ\", \"ರುತ್ತೇವೆ\", \"ರುವುದಿಲ್ಲ\", \"ರುತ್ತೇವ\", \"ರುತ್ತೆವ\", \"ತ್ತೆವೆ\", \"ತ್ತೇವೆ\", \"ವುದಿಲ್ಲ\", \"ತ್ತೇವ\", \"ತ್ತೆವ\"],\n",
    "\n",
    "    # simple 2nd\n",
    "    23: [\"ತ್ತೀಯ\", \"ವುದಿಲ್ಲ\", \"ತ್ತಿಯ\"],\n",
    "\n",
    "    # simple 3rd plural\n",
    "    24: [\"ತ್ತಾರೆ\", \"ತ್ತಾರ\"],\n",
    "\n",
    "    # simple 3rd singular\n",
    "    25: [\"ತ್ತಾನೆ\", \"ತ್ತಾಳೆ\", \"ವುದಿಲ್ಲ\"],\n",
    "\n",
    "    # Present perfect 1st singular\n",
    "    26: [\"ದ್ದini\", \"ದ್ದೆನೆ\", \"ದಿಲ್ಲ\", \"ತ್ತಿದ್ದೆ\", \"ಲ್ಲವ\", \"ದೆನ\"],\n",
    "\n",
    "    # present perfect 1st plural\n",
    "    27: [\"ದ್ದೆವೆ\", \"ದ್ದೆವ\"],\n",
    "\n",
    "    # present perfect 2nd\n",
    "    28: [\"ಡಿದ್ದೀಯ\"],\n",
    "\n",
    "    # present perfect 3rd plural\n",
    "    29: [\"ತ್ತಿದ್ದಾರ\", \"ತ್ತಿದ್ದಾರೆ\"],\n",
    "\n",
    "    # present perfect 3rd singular\n",
    "    30: [\"ಯಾಗಿದೆ\", \"ಯಾಗಿಲ್ಲ\"],\n",
    "\n",
    "    # present continuous 1st singluar\n",
    "    31: [\"ತ್ತಿದ್ದೆನೆ\", \"ತ್ತೆನೆ\", \"ತ್ತೇನೆ\", \"ತ್ತಿದ್ದೇನೆ\", \"ತ್ತಿಲ್ಲ\", \"ತ್ತಿದ್ದೆನ\"],\n",
    "\n",
    "    # present cntinouus 1st plural\n",
    "    32: [\"ತ್ತಿದ್ದೇವೆ\", \"ತ್ತೇವೆ\", \"ತ್ತಿಲ್ಲ\", \"ತ್ತಿದ್ದೇವೆ\", \"ತ್ತಿದ್ದೇವ\"],\n",
    "\n",
    "    # present continous 2nd\n",
    "    33: [\"ಯುತ್ತಿದ್ದೀಯ\", \"ಯುತ್ತೀಯ\", \"ಯುತ್ತಿರುವೆ\", \"ಯುತ್ತಿಲ್ಲ\", \"ಯುವುದಿಲ್ಲ\", \"ತ್ತಿದಿಯ\"],\n",
    "\n",
    "    # present ocntinuous 3rd plural\n",
    "    34: [\"ತಿದರೆ\", \"ತ್ತಿದ್ದಾರೆ\", \"ತ್ತಿಲ್ಲ\", \"ತ್ತಿದ್ದಾರ\", \"ತಿರುವರ\"],\n",
    "\n",
    "    # present continuous 3rd singular\n",
    "    35: [\"ತ್ತಿದ್ದಾನೆ\", \"ತ್ತಿದ್ದಾಳೆ\", \"ತ್ತಾನೆ\", \"ತ್ತಾಳೆ\", \"ತ್ತಿದ್ದಾನ\", \"ತ್ತಿದ್ದಾಳ\", \"ತ್ತಿಲ್ಲ\"],\n",
    "\n",
    "    # PRESENT PERFECT continuous tense 1st singular\n",
    "    36: [\"ತ್ತಿದ್ದೀನಿ\", \"ತ್ತಿರುವೆ\", \"ತ್ತಿಲ್ಲ\", \"ತ್ತಿದ್ದೀನಿ\", \"ತ್ತಿಲ್ಲವೆ\", \"ತ್ತಿದ್ದೇನೆ\"],\n",
    "\n",
    "    # present perfect continuous tense 1st plural\n",
    "    37: [\"ತ್ತಿದ್ದೇವೆ\", \"ತ್ತಿರುವ\", \"ತ್ತಿರುವೆವು\", \"ತ್ತಿರುವೆವ\", \"ತ್ತಿದ್ದೇವ\", \"ತ್ತಿದೇವ\", \"ತ್ತಿಲ್ಲವ\", \"ತ್ತಿಲ್ಲವಾ\"],\n",
    "\n",
    "    # present perfect continuous 2nd\n",
    "    38: [\"ತ್ತಿದೀಯ\", \"ತ್ತಿಲ್ಲ\", \"ತ್ತಿರುವೆಯ\", \"ತ್ತಿದ್ದೆಯ\", \"ತ್ತಿಲ್ಲವ\"],\n",
    "\n",
    "    # present perfect continuous 3rd plural\n",
    "    39: [\"ದಲ್ಲಿದೆ\", \"ಯಲ್ಲಿದೆ\", \"ರಲ್ಲಿದೆ\"],\n",
    "\n",
    "    # present perfect continuous 3rd singular\n",
    "    40: [\"ತ್ತಿದ್ದಾನೆ\", \"ತ್ತಿದ್ದಾಳೆ\", \"ತ್ತಿದ್ದಾಳ\", \"ತ್ತಿದ್ದಾನೆ\"],\n",
    "\n",
    "    41: [\"ಯಾದರೆ\", \"ಗಾದರೆ\", \"ವುದಾದರೆ\", \"ದಾದರೆ\"],\n",
    "\n",
    "    42: [\"ಯಾಗಿಯೇ\", \"ಗಾಗಿಯೇ\", \"ದಾಗಿಯೇ\", \"ವಾಗಿಯೇ\"],\n",
    "\n",
    "    43: [\"ವಾದರು\", \"ಗಾದರು\", \"ತಾದರು\", \"ದಾದರು\", \"ಯಾದರು\", \"ರಾದರು\", \"ಲಾದರು\", \"ಳಾದರು\", \"ವಾದರೂ\", \"ಗಾದರೂ\", \"ತಾದರೂ\", \"ದಾದರೂ\",\n",
    "         \"ಯಾದರೂ\", \"ರಾದರೂ\", \"ಲಾದರರೂ\", \"ಳಾದರೂ\"],\n",
    "\n",
    "    44: [\"ತ್ತಿದ್ದರಂತೆ\", \"ದೊಂದಿಗೆ\", \"ಯೊಂದಿಗೆ\", \"ರೊಂದಿಗೆ\"],\n",
    "\n",
    "    45: [\"ಗಿದ್ದನು\", \"ಗಿದ್ದಳು\", \"ಗಿದ್ದರು\", \"ಗಿದ್ದರೂ\", \"ತಾದ್ದನು\", \"ತಾದ್ದಳು\", \"ತಾದ್ದರು\", \"ತಾದ್ದರೂ\", \"ದಾದ್ದನು\", \"ದಾದ್ದಳು\",\n",
    "         \"ದಾದ್ದರು\", \"ದಾದ್ದರೂ\"],\n",
    "\n",
    "    46: [\"ಯೊಂದೆ\", \"ವೊಂದെ\", \"ರೊಂದೆ\", \"ವೊಂದ\", \"ಯೊಂದ\", \"ರೊಂದ\", \"ವುದೇ\"],\n",
    "\n",
    "    47: [\"ಯುವವರ\", \"ರುವವರ\", \"ಸುವವರ\"],\n",
    "\n",
    "    48: [\"ದಲ್ಲೇ\", \"ನಲ್ಲೇ\", \"ನಲ್ಲಿ\", \"ವಲ್ಲಿ\", \"ದಲ್ಲಿ\", \"ದಲ್ಲೂ\", \"ಯಲ್ಲಿ\", \"ರಲ್ಲಿ\", \"ಗಳಲ್ಲಿ\", \"ಳಲ್ಲಿ\", \"ಯಲ್ಲಿನ\"],\n",
    "\n",
    "    49: [\"ವವರು\", \"ಯವರು\", \"ನವರು\", \"ರವರು\", \"ದವರು\", \"ವವ\", \"ಯವ\", \"ನವ\", \"ರವ\", \"ದವ\"],\n",
    "\n",
    "    50: [\"ಗಾಗಿ\", \"ದಾಗಿ\", \"ವಾಗಿ\", \"ರಾಗಿ\", \"ಯಾಗಿ\", \"ತagi\", \"ಕ್ಕಾಗಿ\", \"ವಾಗಿದ್ದು\", \"ವಾಗಿದ್ದ\", \"ಗಾಗಿದ್ದು\", \"ಗಾಗಿದ್ದ\",\n",
    "         \"ರಾಗಿದ್ದು\", \"ರಾಗಿದ್ದ\", \"ದಾಗಿದ್ದು\", \"ದಾಗಿದ್ದ\", \"ತಾಗಿದ್ದು\", \"ತಾಗಿದ್ದ\"],\n",
    "\n",
    "    51: [\"ರನ್ನ\", \"ನನ್ನ\", \"ಯನ್ನ\"],\n",
    "\n",
    "    52: [\"ರನ್ನು\", \"ವನ್ನು\", \"ಯನ್ನು\", \"ಗಳನ್ನೇ\", \"ಗಳನ್ನು\", \"ಳನ್ನು\", \"ದನ್ನು\"],\n",
    "\n",
    "    53: [\"ವಿರುವ\", \"ರುವ\", \"ದ್ದರೆ\", \"ದ್ದारे\"],\n",
    "\n",
    "    54: [\"ತ್ತಾರಂತೆ\", \"ತ್ತಾಳಂತೆ\", \"ತ್ತಾನಂತೆ\", \"ಗಂತೆ\", \"ದ್ದಂತೆ\", \"ದಂತೆ\", \"ನಂತೆ\", \"ರಂತೆ\", \"ಯಂತೆ\", \"ಗಳಂತೆ\", \"ಳಂತೆ\", \"ವಂತೆ\"],\n",
    "\n",
    "    55: [\"ಗಳೆಂದು\", \"ಗಂ\", \"ದ್ದಂ\", \"ದಂ\", \"ಯಂ\", \"ರಂ\", \"ವಂ\", \"ಗಿಂದ\", \"ದಿಂದ\", \"ಯಿಂದ\", \"ರಿಂದ\", \"ನಿಂದ\"],\n",
    "\n",
    "    56: [\"ನಿಗೆ\", \"ರಿಗೆ\", \"ಯಿಗೆ\", \"ಕೆಗೆ\"],\n",
    "\n",
    "    57: [\"ದ್ದೇನೆ\", \"ದ್ದಾನೆ\", \"ದ್ದಾಳೆ\", \"ದ್ದಾರೆ\", \"ದಾಗ\"],\n",
    "\n",
    "    58: [\"ವಿದೆ\", \"ದಿದೆ\", \"ತಿದೆ\", \"ಗಿದೆ\"],\n",
    "\n",
    "    59: [\"ತ್ತಿರು\", \"ವೆಂದು\"],\n",
    "\n",
    "    60: [\"ನನ್ನೂ\", \"ಳನ್ನೂ\", \"ರನ್ನೂ\"],\n",
    "\n",
    "    61: [\"ಯಾಯಿತು\", \"ಗಾಯಿತು\", \"ದಾಯಿತು\"],\n",
    "\n",
    "    62: [\"ದ್ದನು\", \"ದ್ದಳು\", \"ಯಿದ್ದರು\", \"ದ್ದರು\", \"ದ್ದರೂ\", \"ಗಳೇ\", \"ಗಳು\", \"ಗಳ\", \"ಗಳಿ\", \"ದಳು\", \"ದಳ\", \"ವೆನು\", \"ವನು\", \"ವೆವು\",\n",
    "         \"ವಳು\", \"ವಳ\", \"ವುದು\", \"ಲಾಗು\", \"ಗಳಾದ\", \"ಗಳಿಗೆ\"],\n",
    "\n",
    "    63: [\"ವುದಕ್ಕೆ\", \"ಕ್ಕೆ\", \"ಗ್ಗಿ\", \"ದ್ದಿ\", \"ಲ್ಲಿ\", \"ನ್ನು\", \"ತ್ತು\"],\n",
    "\n",
    "    64: [\"ವಾಯಿತು\", \"ಗಾಯಿತು\", \"ದಾಯಿತು\", \"ತಾಯಿತು\", \"ಲಾಯಿತು\", \"ನಾಯಿತು\"],\n",
    "\n",
    "    65: [\"ವಿದ್ದು\", \"ವೆಂದಾಗ\"],\n",
    "\n",
    "    66: [\"ವನ್ನೇ\", \"ವೇಕೆ\"],\n",
    "\n",
    "    67: [\"ರಾದ\", \"ವಾದ\", \"ಗಾದ\", \"ಯಾದ\", \"ರಾಗುವ\"],\n",
    "\n",
    "    68: [\"ವಾದುದು\", \"ರಾದುದು\", \"ಗಾದುದು\", \"ಯಾದುದು\", \"ದಾದುದು\"],\n",
    "\n",
    "    69: [\"ಯಾರು\", \"ದಾರು\", \"ಗಾರು\", \"ರಾರು\"],\n",
    "\n",
    "    70: [\"ಗಳಿಸಿ\", \"ಗಳಿಸು\", \"ಗಳಿವೆ\", \"ಗಳಿವ\", \"ಗಳಿವು\"],\n",
    "\n",
    "    71: [\"ಯು\", \"ದ\", \"ವಿಕೆ\", \"ದೇ\", \"ರು\", \"ಳ\", \"ಳೆ\", \"ಲಿದೆ\", \"ದೆ\", \"ರೆ\", \"ಗೆ\", \"ವೆ\", \"ತೆ\", \"ಗೂ\"],\n",
    "\n",
    "    72: [\"ರದ\", \"ಮದ\", \"ನದ\"],\n",
    "\n",
    "    73: [\"ಡಲು\", \"ಲಾಗುತ್ತದೆ\", \"ಸಲು\", \"ಸಿದ್ದಾಳೆ\", \"ಸಿದಾಗ\", \"ಸಲು\", \"ಸಿದರು\", \"ಸಿದನು\", \"ಸಿದಳು\", \"ಸಿದ್ದೇ\", \"ಕಿದೀನಿ\"]\n",
    "}\n",
    "add_1 = [\"ು\"]\n",
    "def kannada_root(word, inde):\n",
    "    global flag\n",
    "    for L in complex_suffixes[72]:\n",
    "        if len(word) > len(L) + 1:\n",
    "            if word.endswith(L):\n",
    "                inde.append(72)\n",
    "                return (word[:-(len(L) - 1)], inde)\n",
    "    for L in complex_suffixes[73]:\n",
    "        if len(word) > len(L) + 1:\n",
    "            if word.endswith(L):\n",
    "                flag = 1\n",
    "                word = word[:-(len(L) - 1)]\n",
    "                word = word + add_1[0]\n",
    "                inde.append(73)\n",
    "                return (kannada_root(word, inde))\n",
    "    L = 1\n",
    "    while L <= 70:\n",
    "        for suffix in complex_suffixes[L]:\n",
    "            if len(word) > len(suffix) + 1:\n",
    "                if word.endswith(suffix):\n",
    "                    flag = 1\n",
    "                    inde.append(L)\n",
    "                    return (kannada_root(word[:-(len(suffix))], inde))\n",
    "        L = L + 1\n",
    "    if flag == 0:\n",
    "        for L in complex_suffixes[71]:\n",
    "            if len(word) - len(L) > len(L) + 1:\n",
    "                if word.endswith(L):\n",
    "                    inde.append(71)\n",
    "                    return (word[:-(len(L))], inde)\n",
    "    return word, inde\n",
    "\n",
    "flag = 0\n",
    "x = []\n",
    "\n",
    "def stemming(data):\n",
    "    x = data.split()\n",
    "    y = []\n",
    "    for j in range(len(x)):\n",
    "        flag = 0\n",
    "        inde = []\n",
    "        root = x[j]\n",
    "        root, inde = kannada_root(x[j], inde)\n",
    "        y.append(root)\n",
    "    stemmed = ' '.join([str(elem) for elem in y])\n",
    "    return stemmed\n",
    "\n",
    "df['Sentences'] = df['Sentences'].astype(str)\n",
    "\n",
    "\"\"\"#Data Cleaning and Stemming of Dataset\"\"\"\n",
    "df['Sentences'] = df['Sentences'].apply(lambda x: clean(x))\n",
    "df['Sentences'] = df['Sentences'].apply(lambda x: stemming(x))\n",
    "\n",
    "\"\"\"#Drop NaN values from both 'Sentences' and 'Sarcastic' columns\"\"\"\n",
    "df = df.dropna(subset=['Sarcastic', 'Sentences'])\n",
    "\n",
    "\"\"\"#Ensure 'Sarcastic' column is of the correct type\"\"\"\n",
    "df['Sarcastic'] = df['Sarcastic'].astype(int)  # Convert to integer if it's not already\n",
    "\n",
    "\"\"\"#TF-IDF for Feature Extraction\"\"\"\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "\"\"\"#Splitting the Data into Training and Testing\"\"\"\n",
    "X = df['Sentences']\n",
    "y = df['Sarcastic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fit the vectorizer to the training data only\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "\"\"\"#Training of ML Models\"\"\"\n",
    "# Train the models using the transformed data\n",
    "clf1 = LinearSVC(dual=False)\n",
    "clf2 = LogisticRegression()\n",
    "clf3 = SGDClassifier()\n",
    "clf4 = SVC()\n",
    "clf6 = MultinomialNB()\n",
    "clf7 = RandomForestClassifier()\n",
    "clf8 = XGBoost.XGBClassifier()\n",
    "\n",
    "clf1.fit(X_train_tfidf, y_train)\n",
    "clf2.fit(X_train_tfidf, y_train)\n",
    "clf3.fit(X_train_tfidf, y_train)\n",
    "clf4.fit(X_train_tfidf, y_train)\n",
    "clf6.fit(X_train_tfidf, y_train)\n",
    "clf7.fit(X_train_tfidf, y_train)\n",
    "clf8.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\"\"\"#Testing of ML Models\"\"\"\n",
    "y_pred1 = clf1.predict(X_test_tfidf)\n",
    "y_pred2 = clf2.predict(X_test_tfidf)\n",
    "y_pred3 = clf3.predict(X_test_tfidf)\n",
    "y_pred4 = clf4.predict(X_test_tfidf)\n",
    "y_pred6 = clf6.predict(X_test_tfidf)\n",
    "y_pred7 = clf7.predict(X_test_tfidf)\n",
    "y_pred8 = clf8.predict(X_test_tfidf)\n",
    "\n",
    "\"\"\"#Classification Report\"\"\"\n",
    "print(\"\\t\\t\\tLinear SVC\\n\\n\", classification_report(y_test, y_pred1))\n",
    "print(\"\\t\\t\\tLogisticRegression\\n\\n\", classification_report(y_test, y_pred2))\n",
    "print(\"\\t\\t\\tSGDClassifier\\n\\n\", classification_report(y_test, y_pred3))\n",
    "print(\"\\t\\t\\tSVC\\n\\n\", classification_report(y_test, y_pred4))\n",
    "print(\"\\t\\t\\tMultinomialNB\\n\\n\", classification_report(y_test, y_pred6))\n",
    "print(\"\\t\\t\\tRandomForestClassifier\\n\\n\", classification_report(y_test, y_pred7))\n",
    "print(\"\\t\\t\\tXGBoost\\n\\n\", classification_report(y_test, y_pred8))\n",
    "\n",
    "\"\"\"#Confusion Matrix\"\"\"\n",
    "print(\"Linear SVC\")\n",
    "confusion_matrix(y_test, y_pred1)\n",
    "\n",
    "print(\"LogisticRegression\")\n",
    "confusion_matrix(y_test, y_pred2)\n",
    "\n",
    "print('SGDClassifier')\n",
    "confusion_matrix(y_test, y_pred3)\n",
    "\n",
    "print('SVC')\n",
    "confusion_matrix(y_test, y_pred4)\n",
    "\n",
    "print('MultinomialNB')\n",
    "confusion_matrix(y_test, y_pred6)\n",
    "\n",
    "print('RandomForestClassifier')\n",
    "confusion_matrix(y_test, y_pred7)\n",
    "\n",
    "print('XGBoost')\n",
    "confusion_matrix(y_test, y_pred8)\n",
    "\n",
    "\"\"\"#Accuracy Score\"\"\"\n",
    "accuracy_LinearSVC = accuracy_score(y_test, y_pred1) * 100\n",
    "accuracy_LogisticRegression = accuracy_score(y_test, y_pred2) * 100\n",
    "accuracy_SGDClassifier = accuracy_score(y_test, y_pred3) * 100\n",
    "accuracy_SVC = accuracy_score(y_test, y_pred4) * 100\n",
    "accuracy_MultinomialNB = accuracy_score(y_test, y_pred6) * 100\n",
    "accuracy_RandomForestClassifier = accuracy_score(y_test, y_pred7) * 100\n",
    "accuracy_XGBoost = accuracy_score(y_test, y_pred8) * 100\n",
    "\n",
    "print(\"Linear SVC\")\n",
    "print(round(accuracy_LinearSVC, 2), \"%\\n\")\n",
    "\n",
    "print(\"LogisticRegression\")\n",
    "print(round(accuracy_LogisticRegression,2), \"%\\n\")\n",
    "\n",
    "print('SGDClassifier')\n",
    "print(round(accuracy_SGDClassifier,2), \"%\\n\")\n",
    "\n",
    "print('SVC')\n",
    "print(round(accuracy_SVC,2), \"%\\n\")\n",
    "\n",
    "print('MultinomialNB')\n",
    "print(round(accuracy_MultinomialNB,2), \"%\\n\")\n",
    "\n",
    "print('RandomForestClassifier')\n",
    "print(round(accuracy_RandomForestClassifier,2), \"%\\n\")\n",
    "\n",
    "print('XGBoost')\n",
    "print(round(accuracy_XGBoost,2), \"%\\n\")\n",
    "\n",
    "\"\"\"#BERT Implementation\"\"\"\n",
    "# Tokenize data for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_data(texts, labels, max_len=128): # Reduced max_len\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = [int(label) for label in labels]\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(df['Sentences'].tolist(), df['Sarcastic'].tolist())\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_data(df['Sentences'].tolist(), df['Sarcastic'].tolist())\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Increased batch size\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Increased batch size\n",
    "\n",
    "# Load BERT model\n",
    "# Fine-tune a pre-trained BERT model\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
    "# If using bert-base-uncased\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "#  num_labels should be set to 2 for your binary classification task\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs  # Total steps = batches per epoch * number of epochs\n",
    "accumulation_steps = 2 # Accumulate gradients over 2 batches\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=total_steps\n",
    "      )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss / accumulation_steps # Normalize loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0: # Update parameters every accumulation_steps\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} - Average training loss: {avg_train_loss}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_BERT = accuracy_score(true_labels, pred_labels) * 100\n",
    "print(f\"BERT Accuracy: {accuracy_BERT:.2f}%\")\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "\"\"\"#Custom User Input\"\"\"\n",
    "print(\"Enter a Kannada sentence\")\n",
    "x = input()\n",
    "x = clean(x)\n",
    "print(\"After Cleaning\")\n",
    "print(x,'\\n')\n",
    "print(\"After Stemming\")\n",
    "x = stemming(x)\n",
    "print(x)\n",
    "\n",
    "vec = tfidf.transform([x])\n",
    "vec.shape\n",
    "\n",
    "\"\"\"#Testing User Input On All Classifiers\"\"\"\n",
    "result1=clf1.predict(vec)\n",
    "result2=clf2.predict(vec)\n",
    "result3=clf3.predict(vec)\n",
    "result4=clf4.predict(vec)\n",
    "result6=clf6.predict(vec)\n",
    "result7=clf7.predict(vec)\n",
    "result8=clf8.predict(vec)\n",
    "res=[bool(result1[0]),bool(result2[0]),bool(result3[0]),bool(result4[0]),bool(result6[0]),bool(result7[0]),bool(result8[0])]\n",
    "print(res)\n",
    "\n",
    "\"\"\"#BERT Prediction on Custom Input\"\"\"\n",
    "def predict_sarcasm(text):\n",
    "    model.eval()\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_label = np.argmax(logits, axis=1)\n",
    "    return pred_label[0]\n",
    "\n",
    "result_bert = predict_sarcasm(x)\n",
    "print(\"BERT Prediction:\", result_bert)\n",
    "\n",
    "\"\"\"#Comparing Accuracies\"\"\"\n",
    "Accuracy = [accuracy_LinearSVC, accuracy_LogisticRegression, accuracy_SGDClassifier, accuracy_SVC,\n",
    "            accuracy_MultinomialNB, accuracy_RandomForestClassifier, accuracy_XGBoost, accuracy_BERT]\n",
    "Methods = ['Linear SVC', 'Logistic Regression', 'SGD', 'SVC', 'Multinomial NB', 'Random Forest', 'XGBoost', 'BERT']\n",
    "Accuracy_pos = np.arange(len(Methods))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(Accuracy_pos, Accuracy)\n",
    "plt.xticks(Accuracy_pos, Methods)\n",
    "plt.title('Comparing the accuracy of each model')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3486a25-0fd2-4bbb-9320-b272401c7b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
